The “Advanced Retrieval-Augmented Generation (RAG) for BRIAN” architecture is designed to deliver fast, accurate, and context-aware answers by combining intelligent query expansion, high-performance vector search, content deduplication, and large-scale language model synthesis. This end-to-end workflow ensures that user queries are thoroughly interpreted, the most relevant information is retrieved, and the final response is both precise and comprehensive. The following sections describe the complete flow in detail so that any engineer or stakeholder can easily understand how the system functions from input to output.

The process begins at the Frontend, where a user types a question or request. This interface can be a web application, chatbot window, or any client-facing UI capable of sending the query to the backend. Once the user submits the query, it is forwarded to the backend via an API call. At this point, the system transitions into the core of the RAG pipeline.

The first major step happens within the Azure OpenAI Backend. Instead of relying on a single phrasing of the user’s question, the backend generates four additional variations of the query, alongside the original input. This method, known as query expansion, significantly increases the chances of retrieving the most relevant information from the knowledge base. By rephrasing the user’s intent in multiple ways, the system captures nuances, alternative terminology, and contextual hints that may otherwise be missed.

These five queries—one original and four variants—are then passed into the Azure PostgreSQL Vector Database, which has been optimized for high-dimensional vector similarity search. Each query is converted into an embedding, and the database performs similarity matching to retrieve the top five most relevant text chunks for each version. This ensures that the system gathers a diverse yet targeted set of content related to the user’s needs. The retrieved chunks are then forwarded to the next stage.

Since multiple queries may lead to overlapping or redundant results, the Deduplication Engine becomes essential. Its purpose is to analyze the retrieved chunks, identify duplicates or highly similar pieces of content, and produce a refined set of unique and meaningful contexts. This step ensures efficiency and prevents the language model from being overwhelmed or misled by repetitive information.

After deduplication, the curated set of unique context chunks is handed over to the Azure-hosted LLM Engine. Here, the language model reads the content, interprets the user’s original question, and synthesizes a final, coherent, and well-grounded answer. The LLM leverages both the user’s intent and the supporting knowledge to generate a high-quality response.

Finally, the synthesized answer is returned to the Frontend, where it is displayed to the user. The result is a polished, evidence-backed response enhanced by retrieval and intelligent processing at every stage.

This advanced RAG workflow ensures reliability, accuracy, and a deeply contextual understanding, making the BRIAN system an effective tool for complex information retrieval and generation tasks.
